---
layout:     post
title:      "从 Zero-shot 到 Fine-tuning"
subtitle:   "我的 CLIP 医学影像实验记录"
date:       2025-08-13 14:01:00
author:     "ZhB, Dong"
header-img: " "
catalog: true
tags:
    - Deep Learning
    - Vision Language Model
    - Medical Image
---


# 从 Zero-shot 到 Fine-tuning —— 我的 CLIP 医学影像实验记录
## Abstract
本博客对 OpenAI CLIP 模型在医学 X 光片部位分类任务中的表现进行了评估和微调。首先在 UNIFESP Body Part Classification 数据集上进行Zero-shot测试，模型准确率为 0.60。随后使用其中 1200 张图像对 CLIP 的 image encoder 与 text encoder 进行联合微调，采用 cross-entropy 作为损失函数。在相同数据集的 400 张测试集上，准确率从 0.62 提升至 0.78。为了进一步验证泛化能力，将微调后的模型直接应用于 IRMA 数据集，准确率由微调前的 0.02 提升至 0.88。实验结果表明，虽然 CLIP 在专业医学影像上的零样本性能有限，但通过小规模标注数据的联合微调，可以显著提升模型的领域适应性与跨数据集泛化能力。

## 1. Introduction
Contrastive Language-Image Pre-Training (CLIP) 是由OpenAI在2021年发布的一种多模态模型 [[1]](https://arxiv.org/abs/2103.00020)。
该模型的核心思想是使用大量图像和文本的配对数据进行预训练，以学习图像和文本之间的对齐关系。该模型结构如图所示

![CLIP](./figures/clip.png "CLIP Model")

该模型的核心思想是使用大量图像和文本的配对数据进行预训练 [[2]](https://blog.csdn.net/weixin_47228643/article/details/136690837)，以学习图像和文本之间的对齐关系。CLIP模型有两个模态，一个是文本模态，一个是视觉模态，包括两个主要部分：
- Text Encoder：用于将文本转换为低维向量表示-Embeding。
- Image Encoder：用于将图像转换为类似的向量表示-Embedding。

该模型通过最大化图像向量间和目标文本向量的余弦相似度的方式对Image-encoder和Text-encoder进行预训练。

[论文](https://arxiv.org/abs/2103.00020)的实验结果已经表明，该模型在大量自然图片数据集上均具有较好的zero-shot分类效果，这体现了这类模型具有较好的泛化性。
但是作为一个独特的图像类型的医学影像，其包含的语义信息和自然图像具有一定差异。CLIP能否在医学影像上也展现良好的泛化性，以及对CLIP进行小样本的微调能否显著提升其在医学场景下的性能仍有待进一步实验探究。

本博客主要记录了我在医学影像分类任务中应用 CLIP的主要探索及其结果。
- 1. 我首先探索了在胸部 X 光片（Chest X-ray）数据集上的 zero-shot 预测能力，并进一步进行了微调实验。
- 2. 我通过对 UNIFESP 数据集和 IRMA 数据集的测试，展示了微调前后模型性能的提升，并深入分析了不同学习率对训练过程和最终模型效果的影响。
- 3. 同时，我比较了在CLIP预训练模型使用不同的Prompt对分类结果的影响

通过这些探索，我希望能够展现 CLIP 在医学场景中的跨模态迁移能力，以及微调 CLIP 在特定任务中的有效性。

## 2. Datasets
医学影像的数据集中主要包含疾病标签和器官部位标签两种。由于疾病类数据集（如chetXpert）数据集中影像差异较小，CLIP可能本身无法在zero-shot的情况下对其进行正确分类。而不同的人体部位的Xray图片则具有更大的差异，CLIP更加可能对其进行正确的zero-shot分类。

我主要使用了两个包含多个人体不同部位标签Xray影像的开源数据集UNIFESP和IRMA。
- UNIFESP/train数据集包含1600多张带有15种标签的图像-文本对；
- IRMA/train数据集包含3380多张带有15种标签的图像-文本对；

UNIFESP标签如下
    
    Labels = ['Abdomen', 'Ankle', 'Cervical Spine', 'Chest', 'Clavicles', 'Elbow', 'Feet',
            'Finger', 'Forearm', 'Hand', 'Hip', 'Knee', 'Lower Leg', 'Lumbar Spine', 'Others', 'Pelvis', 'Shoulder',
            'Sinus', 'Skull', 'Thigh', 'Thoracic Spine', 'Wrist']

IRMA标签如下
    
    categories = {
        "1": "whole body",
        "2": "cranium",
        "3": "spine",
        "4": "upper extremity/arm",
        "5": "chest",
        "6": "breast",
        "7": "abdomen",
        "8": "pelvis",
        "9": "lower extremity",
        "10": "chest/bones",
        "11": "chest/lung",
        "12": "chest/hilum",
        "13": "chest/mediastinum",
        "14": "chest/heart",
        "15": "chest/diaphragm"
    }

与UNIFESP相比，IRMA的标签更加关注胸部的细小结构的分类，UNIFESP更加关注人体不同位置的结构分类。因此本实验首先在UNIFESP上测试CLIP的zero-shot性能，并使用该数据对CLIP进行再训练。再在IRMA上测试微调后的模型再胸部细小结构的zero-shot分类性能。

## 3. Experiment and results
一下实验的CLIP模型均指`ViT/32`架构的CLIP模型（即使用vision transformer作为image-encoder， transformer作为text-encoder）。
## 3.1 CLIP在UNIFESP数据集上的Zero-shots测试
构造了如下提示词

|Number|Prompts|
|---|---|
|Prompt1|a X-Ray image of {$NAME} part|
|Prompt2|This is a X-ray image of {$NAME} part. |
|Prompt3|a X-ray of a {$NAME}|
|Prompt4|a X-ray image of {$NAME} of human|
|Prompt5|a image of {$NAME}|

在UNIFESP训练集上进行了Zero-shot测试，测试关键部分代码如下（完整代码在`BodyCLIP.py`）：

    for images, labels in dataloader:
        
        # -- 展示测试进程 --
        if cnt % 10 == 0:
            print(cnt)
        cnt = cnt+1
        image_input = images.to(device)
        
        # -- 获取图片和文本向量 --
        with torch.no_grad():
            image_features = model.encode_image(image_input)
            text_features = model.encode_text(text_inputs)
        image_features /= image_features.norm(dim=-1, keepdim=True)
        text_features /= text_features.norm(dim=-1, keepdim=True)

        # -- 余弦相似度比较 --
        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
        
        # -- 取最高相似度的标签 --
        values, indices = similarity[0].topk(1)
        for indice in indices:
            if labels.item() == indice.item():
        
            # -- 记录正确个数 --
                right_count += 1

Zero-shot分类的Top1准确率如下

|Prompts|Accuracy|
|---|---|
|**Prompt1**|**0.6253**|
|Prompt2|0.5887|
|Prompt3|0.6030|
|Prompt4|0.5974|
|Prompt5|0.5650|

如表所示：
- 1) CLIP在Xray影像的zero-shot的分类已经**具有了一定的准确率（0.56-0.62）**；
- 2) 不同的提示词会使CLIP的分类性能有所不同：仅提及了 'image' 的提示词（prompt5）的准确性最低， **包含了 'X-ray'提示词（prompt1-4）准确率更高**，但是更接近一句话的提示词（prompt2）准确性较低；
- 3) **CLIP的Zero-shot的准确率仍十分有限**。

## 3.2 利用UNIFESP数据对CLIP进行训练与评估
将UNIFESP集的前1200张图片作为训练集，后400张图片作为测试集。首先利用训练集，对CLIP进行再训练 (代码见`CLIPfinetune.py`),然后在验证集上对准确性进行评估。使用Adam优化器对损失函数进行优化，使用交叉熵损失作为损失函数。训练在RTX4090上进行，占用约2G显存。

    for epoch in range(0, epoches):
        scheduler.step()
        total_loss = 0
        batch_num = 0
        # 使用混合精度，占用显存更小
        with torch.cuda.amp.autocast(enabled=True):
            for images, label in your_dataloader:
                images = images.to(device)
                label_tokens = clip.tokenize(label).to(device)
                batch_num += 1
                optimizer.zero_grad()
                with torch.set_grad_enabled(phase == "train"):
                    # -- 计算图片向量和文字向量的相似度 --
                    logits_per_image, logits_per_text = net(images, label_tokens)
                    ground_truth = torch.arange(len(images), dtype=torch.long, device=device)
                    cur_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2
                    total_loss += cur_loss
                    # -- 更新参数 --
                    if phase == "train":
                        cur_loss.backward()
                        if device == "cpu":
                            optimizer.step()
                        else:
                            optimizer.step()
                            clip.model.convert_weights(net)
            epoch_loss = total_loss/dataset_size
            torch.save(net.state_dict(), dirname+f"/{model_name}_epoch_{epoch}.pth")

训练的主要参数如下

    - Adam.lr 1e-6, 1.5e-6, 2e-6
    - Adam.betas 0.9, 0.98
    - Adam.eps 1e-6
    - Adam.weight_decay 0.001
    - Train batch size 16
    - Epoch 50

每个Epoch中模型在训练集上的损失如下

![Train Loss](./figures/loss.png "Train Loss")

在测试集的准确率如下

![Accuracy](./figures/top1accu.png "Accuracy")

如图所示，无论学习率如何，模型的平均损失均在前5个epoch迅速下降，然后在后续5个epoch中继续缓慢下降，随后开始波动。随着训练的进行，准确率逐渐提升，前10个epoch提升幅度较大。使用较大的学习率的训练Loss下降的稍快，准确性显著优于较小学习率的训练。

## 3.3 在IRMA数据集上对训练后的CLIP进行评估
在完成对CLIP在医学影像场景下的微调后，利用IMRA数据集对其进行评估。准确性如表所示：

|Model|Accuracy(%)|
|---|---|
|pre-trained|2|
|lr=1e-6|49|
|lr=1.5e-6|88|
|lr=2e-6|83|

如表所示：
- 1) 当聚焦于肺部细分结构时，预训练模型（pre-trained）预测的准确率相较于UNIFESP数据集显著下降。
- 2) 当学习率较小(lr=1e-6)时, 模型对人体结构的分类性能显著弱于具有较大学习率训练的模型，结合3.2部分的loss和accuracy可知，模型loss没有显著下降，accuracy没有显著提升，似乎达到了局部最小值。
- 3) lr=1.5e-6和lr=2e-6相比，准确率略高，但2e-6的学习率训练的模型在UNIFESP数据集上具有显著更高的准确率。这也许表明虽然其具有更高的分类准确性，但是具有较差的泛化性。这也可能是由于其在大尺度结构上出现了过拟合（即模型更加关注大尺度，而在肺部小尺度结构上的区分度较差）。

## 4. Conclusion
本博客的实验可以得出如下结论：

1. CLIP在医学场景下具有一定的泛化性，在处理医学Zero-shot任务时已经具备了较好的准确率；
2. 原始的CLIP在对细小结构分类性能较差；
3. 利用医学影像模态的少量数据(<2k张)对其进行微调可以显著提升其分类的准确率；
4. 在对VLM模型进行微调时，选择合适的学习率对模型的泛化性影响很大。
5. 不同的Prompt构成也会对分类结果产生影响。

在后续还可以基于CLIP模型进行如下工作：

1. 使用LoRA等方式对CLIP进行微调，并于直接训练CLIP的性能进行比较
2. 比较CLIP在其他医学影像模态的泛化性能。 


