---
layout:     post
title:      "From Zero-shot to Fine-tuning"
subtitle:   "My CLIP Medical Imaging Experiment Blog"
date:       2025-08-13 14:01:00
author:     "ZhB, Dong"
header-img: " "
catalog: true
tags:
    - Deep Learning
    - Vision Language Model
    - Medical Image
---

# From Zero-shot to Fine-tuning — My CLIP Medical Imaging Experiment Blog

## Abstract  
This blog evaluates and fine-tunes the OpenAI CLIP model for medical X-ray body part classification tasks. We first conducted a zero-shot test on the UNIFESP Body Part Classification dataset, where the model achieved an accuracy of 0.60. Then, we performed joint fine-tuning of CLIP’s image encoder and text encoder using 1,200 images from the dataset, with cross-entropy as the loss function. On the same dataset’s 400-image test set, accuracy improved from 0.62 to 0.78. To further verify generalization capability, we directly applied the fine-tuned model to the IRMA dataset, where accuracy increased from 0.02 to 0.88 compared to the pre-fine-tuning model. These results demonstrate that while CLIP’s zero-shot performance on specialized medical imaging is limited, joint fine-tuning with a small amount of labeled data can significantly enhance domain adaptation and cross-dataset generalization.  

## 1. Introduction  
Contrastive Language-Image Pre-Training (CLIP) is a multimodal model released by OpenAI in 2021 [[1]](https://arxiv.org/abs/2103.00020).  
Its core idea is to pre-train on a large corpus of image–text pairs to learn the alignment between images and text. The model architecture is shown below:  

![CLIP](https://github.com/ZhB-Dong/ZhB-Dong.github.io/raw/f7167628bd2fc6a54a0abef985cd0ec6aa50557e/img/CLIP/clip.png "CLIP Model")  

CLIP consists of two modalities: a text modality and a vision modality, each with a dedicated encoder:  
- **Text Encoder**: Converts text into a low-dimensional embedding vector.  
- **Image Encoder**: Converts images into a similar embedding vector.  

The model is trained by maximizing the cosine similarity between image embeddings and their corresponding text embeddings, thereby jointly learning the parameters of both the image encoder and the text encoder [[2]](https://blog.csdn.net/weixin_47228643/article/details/136690837).  

The experimental results in the [CLIP paper](https://arxiv.org/abs/2103.00020) have shown that the model achieves strong zero-shot classification performance across a wide range of natural image datasets, demonstrating its good generalization capability.  

However, medical images, as a unique type of imagery, contain semantic information that differs from that of natural images. Whether CLIP can also exhibit strong generalization in the medical imaging domain, and whether fine-tuning CLIP with a small amount of labeled data can significantly improve its performance in medical scenarios, still require further experimental investigation.  

This blog mainly records my key explorations and findings when applying CLIP to medical image classification tasks:  
1. I first explored CLIP’s zero-shot prediction capability on chest X-ray datasets and then conducted fine-tuning experiments.  
2. I evaluated and compared the model’s performance before and after fine-tuning on the UNIFESP and IRMA datasets, and analyzed the impact of different learning rates on the training process and final performance.  
3. I compared the effects of different prompts used with the CLIP pre-trained model on classification outcomes.  

Through these explorations, I aim to demonstrate CLIP’s cross-modal transferability in medical imaging and the effectiveness of fine-tuning CLIP for specific tasks.  

## 2. Datasets  
Medical imaging datasets mainly include two types of labels: **disease labels** and **body part labels**.  
For disease-type datasets (such as CheXpert), the visual differences between images are relatively small, which may make it difficult for CLIP to correctly classify them in a zero-shot setting.  
In contrast, X-ray images of different body parts vary much more, making zero-shot classification by CLIP more feasible.  

I primarily used two open-source datasets containing X-ray images of multiple different human body parts: **UNIFESP** and **IRMA**.  

- **UNIFESP/train** contains over 1,600 image–text pairs with 15 different labels.  
- **IRMA/train** contains over 3,380 image–text pairs with 15 different labels.  

**UNIFESP labels:**  

    Labels = [
        'Abdomen', 'Ankle', 'Cervical Spine', 'Chest', 'Clavicles', 'Elbow', 'Feet',
        'Finger', 'Forearm', 'Hand', 'Hip', 'Knee', 'Lower Leg', 'Lumbar Spine', 'Others', 
        'Pelvis', 'Shoulder', 'Sinus', 'Skull', 'Thigh', 'Thoracic Spine', 'Wrist'
    ]

**IRMA labels:**

    categories = {
        "1": "whole body",
        "2": "cranium",
        "3": "spine",
        "4": "upper extremity/arm",
        "5": "chest",
        "6": "breast",
        "7": "abdomen",
        "8": "pelvis",
        "9": "lower extremity",
        "10": "chest/bones",
        "11": "chest/lung",
        "12": "chest/hilum",
        "13": "chest/mediastinum",
        "14": "chest/heart",
        "15": "chest/diaphragm"
    }

Compared with UNIFESP, the IRMA dataset focuses more on the classification of fine-grained structures within the chest, while UNIFESP focuses more on structural classification across different body locations.

Therefore, in this study, I first evaluated CLIP’s zero-shot performance on UNIFESP and used this dataset for fine-tuning. Then, I tested the fine-tuned model on IRMA to assess its zero-shot classification capability for fine-grained chest structures.

## 3. Experiment and Results
All CLIP models used in the following experiments refer to the `ViT/32` architecture (i.e., using a Vision Transformer as the image encoder and a Transformer as the text encoder).  

### 3.1 Zero-shot Testing of CLIP on the UNIFESP Dataset
The following prompts were constructed:  

| Number | Prompts |
|---|---|
| Prompt1 | a X-Ray image of {$NAME} part |
| Prompt2 | This is a X-ray image of {$NAME} part. |
| Prompt3 | a X-ray of a {$NAME} |
| Prompt4 | a X-ray image of {$NAME} of human |
| Prompt5 | a image of {$NAME} |

Zero-shot testing was conducted on the UNIFESP training set. The key testing code is shown below (full code in `BodyCLIP.py`):

for images, labels in dataloader:
        
        # -- 展示测试进程 --
        if cnt % 10 == 0:
            print(cnt)
        cnt = cnt+1
        image_input = images.to(device)
        
        # -- 获取图片和文本向量 --
        with torch.no_grad():
            image_features = model.encode_image(image_input)
            text_features = model.encode_text(text_inputs)
        image_features /= image_features.norm(dim=-1, keepdim=True)
        text_features /= text_features.norm(dim=-1, keepdim=True)

        # -- 余弦相似度比较 --
        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
        
        # -- 取最高相似度的标签 --
        values, indices = similarity[0].topk(1)
        for indice in indices:
            if labels.item() == indice.item():
        
            # -- 记录正确个数 --
                right_count += 1
    
The Top-1 zero-shot classification accuracy is as follows:

| Prompts | Accuracy |
|---|---|
| **Prompt1** | **0.6253** |
| Prompt2 | 0.5887 |
| Prompt3 | 0.6030 |
| Prompt4 | 0.5974 |
| Prompt5 | 0.5650 |

As shown in the table:  
1) CLIP already achieves **a certain level of accuracy (0.56–0.62)** in zero-shot classification of X-ray images.  
2) Different prompts result in different classification performances: prompts mentioning only "image" (Prompt5) yield the lowest accuracy, while **prompts containing "X-ray" (Prompt1–4) achieve higher accuracy**. However, more sentence-like prompts (Prompt2) show slightly lower accuracy.  
3) **The zero-shot accuracy of CLIP on X-ray images remains limited**.  

## 3.2 Fine-tuning CLIP with the UNIFESP Dataset
The first 1,200 images from the UNIFESP dataset were used as the training set, and the remaining 400 images as the test set. The CLIP model was fine-tuned using the training set (code available in `CLIPfinetune.py`), and its accuracy was evaluated on the test set.  
The Adam optimizer was used to optimize the loss function, with cross-entropy loss as the objective.  
Training was conducted on an RTX 4090 GPU, consuming approximately 2 GB of VRAM.

    for epoch in range(0, epoches):
        scheduler.step()
        total_loss = 0
        batch_num = 0
        # 使用混合精度，占用显存更小
        with torch.cuda.amp.autocast(enabled=True):
            for images, label in your_dataloader:
                images = images.to(device)
                label_tokens = clip.tokenize(label).to(device)
                batch_num += 1
                optimizer.zero_grad()
                with torch.set_grad_enabled(phase == "train"):
                    # -- 计算图片向量和文字向量的相似度 --
                    logits_per_image, logits_per_text = net(images, label_tokens)
                    ground_truth = torch.arange(len(images), dtype=torch.long, device=device)
                    cur_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2
                    total_loss += cur_loss
                    # -- 更新参数 --
                    if phase == "train":
                        cur_loss.backward()
                        if device == "cpu":
                            optimizer.step()
                        else:
                            optimizer.step()
                            clip.model.convert_weights(net)
            epoch_loss = total_loss/dataset_size
            torch.save(net.state_dict(), dirname+f"/{model_name}_epoch_{epoch}.pth")

The parameters of training is as follow:

    - Adam.lr 1e-6, 1.5e-6, 2e-6
    - Adam.betas 0.9, 0.98
    - Adam.eps 1e-6
    - Adam.weight_decay 0.001
    - Train batch size 16
    - Epoch 50

    The training loss of the model on the training set for each epoch is shown below:

![Train Loss](https://github.com/ZhB-Dong/ZhB-Dong.github.io/raw/f7167628bd2fc6a54a0abef985cd0ec6aa50557e/img/CLIP/loss.png "Train Loss")

The accuracy on the test set is shown below:

![Accuracy](https://github.com/ZhB-Dong/ZhB-Dong.github.io/raw/f7167628bd2fc6a54a0abef985cd0ec6aa50557e/img/CLIP/top1accu.png "Accuracy")

As shown in the figures, regardless of the learning rate, the average training loss drops rapidly within the first 5 epochs, then continues to decline slowly over the next 5 epochs before starting to fluctuate. As training progresses, accuracy gradually improves, with a significant increase in the first 10 epochs. Training with a larger learning rate results in a slightly faster loss decrease and significantly better accuracy compared to smaller learning rates.

## 3.3 Evaluation of the Fine-tuned CLIP on the IRMA Dataset

After fine-tuning CLIP in the medical imaging domain, the model was evaluated on the IRMA dataset. The accuracies are listed as follows:

| Model       | Accuracy (%) |
|-------------|--------------|
| pre-trained | 2            |
| lr=1e-6     | 49           |
| lr=1.5e-6   | 88           |
| lr=2e-6     | 83           |

As the table shows:  
1) When focusing on fine-grained lung structures, the pre-trained model's accuracy drops significantly compared to its performance on the UNIFESP dataset.

2) With a smaller learning rate (lr=1e-6), the model’s classification performance on anatomical structures is significantly worse than models trained with larger learning rates. Combined with the loss and accuracy trends from section 3.2, the model's loss does not improve significantly and accuracy stagnates, suggesting it may have reached a local minimum.  

3) Although the model trained with lr=1.5e-6 achieves slightly higher accuracy than lr=2e-6, the lr=2e-6 model has significantly higher accuracy on the UNIFESP dataset. This may indicate that while it achieves higher classification accuracy, its generalization ability is weaker. This could be due to overfitting on large-scale structures (the model focuses more on large-scale features and less on distinguishing small-scale lung structures).

## 4. Conclusion

The experiments in this blog lead to the following conclusions:

1. CLIP demonstrates a certain degree of generalization in medical scenarios and achieves reasonable accuracy on medical zero-shot tasks.  
2. The original CLIP performs poorly on fine-grained structure classification.  
3. Fine-tuning with a small amount of medical imaging data (< 2,000 images) significantly improves classification accuracy.  
4. Choosing an appropriate learning rate during VLM fine-tuning greatly affects the model’s generalization ability.  
5. Different prompt designs also impact classification results.

Future work based on the CLIP model could include:

1. Fine-tuning CLIP using methods like LoRA and comparing performance with direct CLIP training.  
2. Comparing CLIP’s generalization performance across other medical imaging modalities.
